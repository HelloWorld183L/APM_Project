# Pro Data Scientists - Applied Predictive Modelling Project

### Team members

Leon Harper (21385662)

Thomas Newton (21365654)

Michal Jedruszczak (21440496)

# Initial set-up

## Libraries to use
```{r}
install.packages("dplyr")
install.packages("ggplot2")
install.packages("purrr")
install.packages("stopwords")
install.packages("textstem")
install.packages("rvest")
install.packages("ROSE")
install.packages("tm")
cat("\014")
```

```{r}
library(dplyr)
library(ggplot2)
library(xml2)
library(purrr)
library(stringr)
library(stopwords)
library(textstem)
library(rvest)
library(ROSE)
library(tm)
```

# World of Warcraft Cyberbullying Dataset analysis

## Step 1: Problem statement
World of Warcraft is a popular MMORPG (Massively multiplayer online role-playing game) video game with millions of user every month. Due to having this many players there is bound to be some cyberbullying/toxic players included in those millions of players, World of Warcraft however is especially toxic and is often ranked as one of the most toxic gaming communityâ€™s today. In a survey by ADL (Anti-Defamation League) it was found 66% of adults ages 18-45 have been harassed/bullied in World of Warcraft in 2021.

The objective of this project is to create a model that will be able to detect cyberbullying/toxicity. By using the World of Warcraft dataset provided to us it will allow the model to have reference for comments deemed as bullying.

This would be a classification model that when give a comment/statement would decided whether it is bullying or not bullying, it will be able to do this by detecting certain words and phrases.

Currently most video games have an option to filter chat, however this only censors certain words/phrases. Our model will be able to detect strings of words rather than just certain ones.

ADL Survey: Hate is No Game: Harassment and Positive Social Experiences in Online Games 2021 (adl.org)


## Step 2: Importing data
```{r}
wow_posts_df <- read.csv("Data/posts_wow.csv")
wow_annotations_df <- read.csv("Data/annotations_wow.csv")

lol_posts_df <- read.csv("Data/posts_lol.csv")
lol_annotations_df <- read.csv("Data/annotations_lol.csv")
```

## Step 2.5: Preliminary EDA
We are doing a preliminary EDA in order to understand how we should clean the data and the kind of data that we are dealing with.

## Step 3: Cleaning the data / Pre-processing
```{r}
wow_posts_df$dataset <- "WoW"
lol_posts_df$dataset <- "LoL"
wow_annotations_df$dataset <- "WoW"
lol_annotations_df$dataset <- "LoL"

posts_df <- rbind(wow_posts_df, lol_posts_df)
annotations_df <- rbind(lol_annotations_df, wow_annotations_df)
```

```{r}
posts_df$id <- paste(posts_df$dataset, posts_df$topic_id, posts_df$post_number, sep="_")
annotations_df$id <- paste(annotations_df$dataset, annotations_df$topic_id, annotations_df$post_number, sep="_")
```

```{r}
merged_df <- left_join(posts_df, annotations_df, by = "id", keep=TRUE)
merged_df$is_bullying <- as.integer(!is.na(merged_df["id.y"]))
drop <- c('topic_id.y', 'post_number.y', 'dataset.y', 'id.y', 'offender', 'victim')
merged_df <- merged_df[, !(names(merged_df) %in% drop)]

colnames(merged_df) = sub(".x", "", colnames(merged_df))

names(merged_df)[names(merged_df) == "annotator"] <- "bullying_severity"
merged_df["bullying_severity"][is.na(merged_df["bullying_severity"])] <- 0
posts_df <- merged_df %>% group_by(id) %>% slice(which.max(bullying_severity))
```

```{r}
stop_words <- stopwords()
'%nin%' <- Negate('%in%')

remove_html <- function(html_msg, isHtml, dataset) {
  if(isHtml) {
    html_msg <- gsub("\\\\", '', html_msg)
    msg <- xml2::read_html(html_msg)
    blockquotes <- msg %>% html_nodes("blockquote")
    quotes <- msg %>% html_nodes(".quote")
    
    xml_remove(blockquotes)
    xml_remove(quotes)
    msg <- html_text(msg)
    return(msg)
  }
  return(html_msg)
}
```

```{r}
detect_html_regex <- "<.*?>"
posts_df$is_html <- str_detect(posts_df$html_message, detect_html_regex)
posts_df$html_message <- mapply(remove_html, posts_df$html_message, posts_df$is_html, posts_df$dataset)
posts_df <- posts_df[, !(names(posts_df) %in% 'is_html')]

toSpaceTransformer <- content_transformer(function (x, pattern) gsub(pattern, "", x))
posts_corpus <- Corpus(VectorSource(posts_df$html_message))
posts_corpus <- posts_corpus %>% 
                tm_map(content_transformer(tolower)) %>%
                tm_map(toSpaceTransformer, "http\\S+\\s*") %>%
                tm_map(removeNumbers) %>%
                tm_map(removeWords, stopwords("english")) %>%
                tm_map(removePunctuation) %>%
                tm_map(stemDocument) %>%
                tm_map(stripWhitespace)
posts_df$html_message <- data.frame(text=sapply(posts_corpus, identity), stringsAsFactors = F)$text
```

```{r}
posts_df$word_counts <- str_count(posts_df$html_message, "\\S+")
```

```{r}
posts_df <- posts_df %>% na_if("") %>% na.omit
```

```{r}
write.csv(posts_df, file="Data/clean_posts.csv")
```

```{r}
corpus = VCorpus(VectorSource(posts_df$html_message))
dtm = DocumentTermMatrix(corpus)
dtm = removeSparseTerms(dtm, 0.999)
posts_data = as.data.frame(as.matrix(dtm))
posts_data$is_bullying = as.factor(posts_df$is_bullying)
```

```{r}
ggplot(data=posts_data, aes(x=is_bullying)) + geom_bar()
```
As we can see, the data is heavily imbalanced where there isn't many bullying cases. This will result in the classifier being trained to where it is more accurate at classifying non-bullying cases rather than bullying cases. We will use undersampling because we have plenty of non-bullying data but not enough data for bullying cases (this means we can afford to reduce how much data we are dealing with).

```{r}
is_bullying = which(posts_data$is_bullying == 1)
not_bullying = which(posts_data$is_bullying == 0)
nsamp = min(length(is_bullying), length(not_bullying))
sample_bullying = sample(is_bullying, nsamp)
sample_not_bullying = sample(not_bullying, nsamp)
posts_data_balanced = posts_data[c(sample_bullying, sample_not_bullying),]

ggplot(data=posts_data_balanced, aes(x=is_bullying)) + geom_bar()
```

This data should be balanced now. However, this comes at the expense of having much less data to work with.

```{r}
set.seed(42)
part <- sample(2, nrow(posts_data), replace=TRUE, prob=c(0.6, 0.4))
train <- posts_data[part == 1, ]
test <- posts_data[part == 2, ]
```

```{r}
set.seed(42)
part <- sample(2, nrow(posts_data_balanced), replace=TRUE, prob=c(0.6, 0.4))
train_balanced <- posts_data_balanced[part == 1, ]
test_balanced <- posts_data_balanced[part == 2, ]
```

```{r}
write.csv(posts_data, file="Data/clean_posts.csv")
write.csv(posts_data_balanced, file="Data/clean_posts_balanced_sample.csv")
write.csv(train_balanced, file="Data/train_balanced.csv")
write.csv(test_balanced, file="Data/test_balanced.csv")
write.csv(train, file="Data/train.csv")
write.csv(test, file="Data/test.csv")
```
## Step 4: EDA

## Step 5: Feature selection

## Step 6: Predictive modelling

```{r}

```

## Step 7: Evaluation

## Step 8: Hyperparameter tuning

# Individual contributions

# References
Bretschneider, Uwe and Peters, Ralf, "DETECTING CYBERBULLYING IN ONLINE COMMUNITIES" (2016). Research Papers. Paper 61. http://aisel.aisnet.org/ecis2016_rp/61