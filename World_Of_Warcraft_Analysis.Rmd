# Pro Data Scientists - Applied Predictive Modelling Project

### Team members

#### Leon Harper (21385662)

#### Thomas Newton (21365654)

#### Michal Jedruszczak (21440496)

These are the team members for the group project.

# Initial set-up

## Libraries to use
```{r}
install.packages("dplyr")
install.packages("ggplot2")
install.packages("purrr")
install.packages("stopwords")
install.packages("textstem")
install.packages("rvest")
install.packages("ROSE")
install.packages("tm")
install.packages("caret")
install.packages("kernlab")
install.packages("RSNNS")
install.packages("randomForest")
install.packages("pROC")
install.packages("tictoc")
install.packages("wordcloud")
install.packages("ggraph")
cat("\014")
```
Run the above block to install the packages necessary to run the code in the project. "cat("\014)" is used to clear the console output for presentation purposes.

```{r}
library(dplyr)
library(ggplot2)
library(xml2)
library(purrr)
library(stringr)
library(stopwords)
library(textstem)
library(rvest)
library(ROSE)
library(tm)
library(caret)
library(kernlab)
library(RSNNS)
library(randomForest)
library(pROC)
library(tictoc)
library(wordcloud)
library(ggraph)
cat("\014")
```
Run the above code block to import the required libraries.

# World of Warcraft Cyberbullying Dataset analysis

## Step 1: Problem statement
World of Warcraft is a popular MMORPG (Massively multiplayer online role-playing game) video game with millions of user every month. Due to having this many players there is bound to be some cyberbullying/toxic players included in those millions of players, World of Warcraft however is especially toxic and is often ranked as one of the most toxic gaming communityâ€™s today. In a survey by ADL (Anti-Defamation League) it was found 66% of adults ages 18-45 have been harassed/bullied in World of Warcraft in 2021 (Hate is No Game: Harassment and Positive Social Experiences in Online Games 2021).

The objective of this project is to create a model that will be able to detect cyberbullying/toxicity. By using the World of Warcraft dataset provided to us it will allow the model to have reference for comments deemed as bullying.

This would be a classification model that when give a comment/statement would decided whether it is bullying or not bullying, it will be able to do this by detecting certain words and phrases.

Currently most video games have an option to filter chat, however this only censors certain words/phrases. Our model will be able to detect strings of words rather than just certain ones.

## Step 2: Importing data
```{r}
wow_posts_df <- read.csv("Data/posts_wow.csv")
wow_annotations_df <- read.csv("Data/annotations_wow.csv")

lol_posts_df <- read.csv("Data/posts_lol.csv")
lol_annotations_df <- read.csv("Data/annotations_lol.csv")
```
This imports the required data for the project. The data was exported from an SQL script that creates the necessary tables (i.e. posts and annotations) and the data. To simplify the process of importing data, we used the table export wizard to export the SQL table data into csv files using custom SQL as the MySQL Workbench Table Export Wizard doesn't export all of the data properly.

## Step 2.5: Preliminary EDA
We are doing a preliminary EDA in order to understand how we should clean the data and the kind of data that we are dealing with.

### League of Legends analysis
```{r}
summary(lol_posts_df)
```

```{r}
nrow(lol_posts_df)
```

There are 16867 data points in the League of Legends posts data.

```{r}
lol_posts_df %>% select("topic_id") -> lol_topics #gets all unique lol_topics from the dataset
nrow(unique(lol_topics))
```
This data set has a total of 17 different topics.

### World of Warcraft analysis
In our World of Warcraft posts dataset, we are given 354 rows of 5 features: topic_id, post_number, author, html_message and timestamp. For the purposes of this project, we will ignore timestamp, as it will not be used to train our models. The additional WoW annotations dataset reveals which messages were flagged as cyberbullying, from which the only useful features are the post_number and topic_id. This information has been combined into a single file -- clean_posts_balanced_sample.csv.

Posts in the main dataset are formatted using HTML, meaning that our model will either have to be trained to recognise patterns such as paragraph breaks or we will have to clean the data and transform it into something more appropriate.

Furthermore, as we are only using a single set of features (post_number and topic_id) from the annotations dataset, we may be able to add another column to the posts dataset -- a Boolean value representing whether or not a specific post contains cyberbullying. This will eliminate the need for the use of two separate datasets to develop our models.

Regarding the types of data we're given, topic IDs, authors and HTML messages are categorical, while post numbers are ordinal. In the annotations data set, all values but post number are categorical.

```{r}
summary(wow_posts_df)
```

```{r}
nrow(wow_posts_df)
```

There are 16978 data points.

```{r}
wow_posts_df %>% select("topic_id") -> wow_topics #gets all unique wow_topics from the dataset
nrow(unique(wow_topics))
```

This data set has a total of 23 different topics.

```{r}
wow_posts_df %>% filter(is_bullying == 1) -> bullying_wow
bullying_wow
```

```{r}
100 * count(bullying_wow)/count(posts_wow)#percentage of posts marked as cyberbullying
```

For this data set 43.8% of the posts are labeled as bullying.

## Step 3: Cleaning the data / Pre-processing
```{r}
# Creates dataset column to merge posts and annotations csv files together
wow_posts_df$dataset <- "WoW"
lol_posts_df$dataset <- "LoL"
wow_annotations_df$dataset <- "WoW"
lol_annotations_df$dataset <- "LoL"

posts_df <- rbind(wow_posts_df, lol_posts_df)
annotations_df <- rbind(lol_annotations_df, wow_annotations_df)
```
Since wow_posts_df and lol_posts_df have the same structure, we merged the posts and annotation data frames together to simplify pre-processing (this avoids repeating code). However, we will need to analyse the datasets separately for EDA purposes so we created a "dataset" feature to counteract this.

```{r}
posts_df$id <- paste(posts_df$dataset, posts_df$topic_id, posts_df$post_number, sep="_")
annotations_df$id <- paste(annotations_df$dataset, annotations_df$topic_id, annotations_df$post_number, sep="_")
```
To simplify the merging of data frames, we will create an ID column so that a left join can be performed on a single column. This mitigates the issues of duplicate topic ids and post numbers as the post numbers are only unique according to the topic id. 

```{r}
merged_df <- left_join(posts_df, annotations_df, by = "id", keep=TRUE)
merged_df$is_bullying <- as.integer(!is.na(merged_df["id.y"]))
drop <- c('topic_id.y', 'post_number.y', 'dataset.y', 'id.y', 'offender', 'victim')
merged_df <- merged_df[, !(names(merged_df) %in% drop)]

# Removes the ".x" characters from the remaining annotations columns
colnames(merged_df) = sub(".x", "", colnames(merged_df))

# Create bullying_severity column
names(merged_df)[names(merged_df) == "annotator"] <- "bullying_severity"
merged_df["bullying_severity"][is.na(merged_df["bullying_severity"])] <- 0
posts_df <- merged_df %>% group_by(id) %>% slice(which.max(bullying_severity))
```
This code performs a left join to merge the dataframes together. Most of the columns from the annotations dataframe are useless for training an NLP classifier so we will be removing those columns. Since there are duplicate columns on each side, we will be dropping "y" columns. 

We also created a bullying_severity column as we found that some posts have been annotated as bullying by multiple annotators which could make this a useful feature for model building.

```{r}
remove_html <- function(html_msg, isHtml) {
  if(isHtml) {
    # Remove backslashes when dealing with LoL forum data
    html_msg <- gsub("\\\\", '', html_msg)
    # Get XML nodes
    msg <- xml2::read_html(html_msg)
    # Get the block quotes and quotes (blockquotes for WoW, .quote for LoL)
    blockquotes <- msg %>% html_nodes("blockquote")
    quotes <- msg %>% html_nodes(".quote")
    
    # Remove quote elements for LoL and WoW datasets
    xml_remove(blockquotes)
    xml_remove(quotes)
    msg <- html_text(msg)
    return(msg)
  }
  return(html_msg)
}
```
The "html_message" column has messages that have HTML and do not contain HTML at all. In order to handle this, we will be creating a "is_html" column that uses a regular expression to detect HTML in order to prevent errors with RVest. The "tm" package does not handle removing HTML content and we cannot simply use a regular expression to remove HTML as the data originates from gaming forums where "<blockquote>" elements are frequently used. If we used a regular expression then the content inside the blockquotes would still remain.

To remove the content of the blockquotes, we used RVest to acquire the blockquote element contents as well as any <div> elements with ".quote" and then we use xml_remove() to remove the blockquote element nodes. We then convert the RVest object back into a string.

```{r}
# Regex for detecting HTML
detect_html_regex <- "<.*?>"
# Create is_html column
posts_df$is_html <- str_detect(posts_df$html_message, detect_html_regex)
# Apply remove_html function to html_message
posts_df$html_message <- mapply(remove_html, posts_df$html_message, posts_df$is_html)
posts_df <- posts_df[, !(names(posts_df) %in% 'is_html')]

# Converts any regex passed into the transformer into a space character
toSpaceTransformer <- content_transformer(function (x, pattern) gsub(pattern, "", x))
posts_corpus <- Corpus(VectorSource(posts_df$html_message))
posts_corpus <- posts_corpus %>% 
                tm_map(content_transformer(tolower)) %>%
                tm_map(toSpaceTransformer, "http\\S+\\s*") %>%
                tm_map(removeNumbers) %>%
                tm_map(removeWords, stopwords("english")) %>%
                tm_map(removePunctuation) %>%
                tm_map(stemDocument) %>%
                tm_map(stripWhitespace)
posts_df$html_message <- data.frame(text=sapply(posts_corpus, identity), stringsAsFactors = F)$text
```
This code removes useless characters, stopwords, punctuation and it uses stemming to improve model performance. Certain steps of the pre-processing could be tweaked to improve model performance (e.g. number of stopwords being omitted) as the pre-processing could end up being too rigorous. 

We removed the HTML characters first in order to prevent interference when removing punctuation or whitespace.

```{r}
posts_df$word_counts <- str_count(posts_df$html_message, "\\S+")
```
This code gets the word counts for the html messages which can be used for analysing word counts in the EDA. We may also use the word counts to filter messages with word counts that are too low.

```{r}
posts_df <- posts_df %>% na_if("") %>% na.omit
```
This code removes NaN rows from posts_df which can become a problem after pre-processing if there were too many stop words in the original messages.

```{r}
write.csv(posts_df, file="Data/clean_posts.csv")
```
This code exports the clean posts to a csv file to be analysed separately. This also comes in handy in order to save time when performing EDAs as pre-processing can take time (especially on slow computers).

```{r}
corpus = VCorpus(VectorSource(posts_df$html_message))
dtm = DocumentTermMatrix(corpus)
dtm = removeSparseTerms(dtm, 0.999)
posts_data = as.data.frame(as.matrix(dtm))
posts_data$is_bullying = as.factor(posts_df$is_bullying)
```
We create a document term matrix from the html messages and we remove sparse terms using removeSparseTerms. We then assign a "is_bullying" column for model building.

```{r}
ggplot(data=posts_data, aes(x=is_bullying)) + geom_bar()
```
As we can see, the data is heavily imbalanced where there isn't many bullying cases. This will result in the classifier being trained to where it is more accurate at classifying non-bullying cases rather than bullying cases. We will use undersampling because we have plenty of non-bullying data but not enough data for bullying cases (this means we can afford to reduce how much data we are dealing with).

```{r}
is_bullying = which(posts_data$is_bullying == 1)
not_bullying = which(posts_data$is_bullying == 0)
nsamp = min(length(is_bullying), length(not_bullying))
sample_bullying = sample(is_bullying, nsamp)
sample_not_bullying = sample(not_bullying, nsamp)
posts_data_balanced = posts_data[c(sample_bullying, sample_not_bullying),]

ggplot(data=posts_data_balanced, aes(x=is_bullying)) + geom_bar()
```
This creates a sample of the bullying data for balancing purposes. However, this comes at the expense of having much less data to work with.

```{r}
set.seed(42)
part <- sample(2, nrow(posts_data), replace=TRUE, prob=c(0.6, 0.4))
train <- posts_data[part == 1, ]
test <- posts_data[part == 2, ]
```
We split the data using a 60:40 split.

```{r}
set.seed(42)
part <- sample(2, nrow(posts_data_balanced), replace=TRUE, prob=c(0.6, 0.4))
train_balanced <- posts_data_balanced[part == 1, ]
test_balanced <- posts_data_balanced[part == 2, ]
```
We split the data using a 60:40 split. This is for the balanced data.

```{r}
write.csv(posts_data_balanced, file="Data/clean_posts_dtm_balanced_sample.csv")
write.csv(train_balanced, file="Data/train_balanced.csv")
write.csv(test_balanced, file="Data/test_balanced.csv")
write.csv(train, file="Data/train.csv")
write.csv(test, file="Data/test.csv")
write.csv(posts_data, file="Data/clean_posts_dtm.csv")
```
We export the training and test data to make model building easier.

## Step 4: EDA
### League of Legends analysis
```{r}
posts_df %>% filter(dataset == "LoL") -> lol_posts
lol_posts %>% filter(is_bullying == 1) -> bullying_lol
bullying_lol
```

```{r}
nrow(bullying_lol) / nrow(lol_posts) * 100

#100 * count(bullying_lol) / count(lol_posts)#percentage of posts marked as cyberbullying
```
For this data set 1.53% of the posts are labeled as bullying.

```{r}
lol_tibble <- tibble(txt = lol_posts$html_message)
lol_tibble #transforming the HTML messages into a tibble for an easier workflow
```

```{r}
lol_tibble <- lol_tibble%>% 
    mutate(linenumber = row_number()) %>%
 unnest_tokens(word, txt) %>% anti_join(stop_words)
lol_tibble #splitting tibble by words
```

```{r}
lol_counts <- lol_tibble %>% count(word, sort=TRUE)
lol_counts #sorting words by count
```

```{r}
wordcloud(lol_counts$word, lol_counts$n, 
          min.freq=25, random.order=FALSE, colors=brewer.pal(8, "Dark2"))
  #creating a word cloud out of sorted list
```

```{r}
lol_bigrams <- tibble(txt = lol_posts$html_message) %>% 
  unnest_tokens(bigram, txt, token = "ngrams", n = 2)
lol_bigrams
#split original tibble into two-word bigrams
```

```{r}
lol_bigrams <- lol_bigrams %>%
      separate(bigram, c("word1", "word2"), sep = " ") 
lol_bigrams #separating them out for easier cleaning
```

```{r}
lol_bigrams <- lol_bigrams %>%
      filter(!word1 %in% stop_words$word) %>%
      filter(!word2 %in% stop_words$word)
lol_bigrams #filtering out unwanted words
```

```{r}
lol_bigrams <- lol_bigrams %>%
            filter(!is.na(word1)) %>% 
            filter(!is.na(word2))
lol_bigrams #removing null values
```

```{r}
lol_bigrams <- lol_bigrams %>%
            unite(bigram, word1, word2, sep=" ")
lol_bigrams #joining the words back together
```

```{r}
lol_bigram_counts <- lol_bigrams %>% count(bigram, sort=TRUE)
lol_bigram_counts #counting and sorting bigrams
```

```{r}
lol_bigram_counts %>% 
  filter(str_detect(lol_bigram_counts$bigram,"[0-9]", negate = TRUE)) -> lol_bigram_counts
lol_bigram_counts #removing any bigrams with numbers
```

```{r}
lol_filtered_bigrams <- lol_bigram_counts %>%
                  filter(n >= 4)
lol_filtered_bigrams #sorting remaining bigrams with a frequency of 4 or more
```

```{r}
lol_separated_bigrams <- lol_filtered_bigrams %>% 
  select("bigram") %>%
  separate(bigram, c("word1", "word2"), sep = " ") 
lol_separated_bigrams #separating bigrams again, preparing for graphical representation
```

```{r}
lol_bigram_graph <- lol_separated_bigrams %>%
                  graph_from_data_frame()
lol_bigram_graph  #creating bigram graph
```

```{r}
ggraph(lol_bigram_graph, layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1)
```

### World of Warcraft analysis
```{r}
wow_posts <- posts_df %>% filter(dataset == "WoW")
posts_df %>% filter(is_bullying == 1) -> bullying_wow
bullying_wow
```

```{r}
nrow(bullying_wow) / nrow(wow_posts) * 100
```

For this data set 2.4% of the posts are labeled as bullying.

```{r}
wow_tibble <- tibble(txt = wow_posts$html_message)
wow_tibble #transforming the HTML messages into a tibble for an easier workflow
```

```{r}
wow_tibble <- wow_tibble%>% 
    mutate(linenumber = row_number()) %>%
 unnest_tokens(word, txt) %>% anti_join(stop_words)
wow_tibble #splitting tibble by words and removing stop words
```

```{r}
wow_counts <- wow_tibble %>% count(word, sort=TRUE)
wow_counts #sorting and counting the remaining words
```

```{r}
wordcloud(wow_counts$word, wow_counts$n, 
          min.freq=25, random.order=FALSE, colors=brewer.pal(8, "Dark2"))
  #creating word cloud
```

```{r}
wow_bigrams <- tibble(txt = wow_posts$html_message) %>% 
  unnest_tokens(bigram, txt, token = "ngrams", n = 2)
wow_bigrams #split original tibble into two-word bigrams
```

```{r}
wow_bigrams <- wow_bigrams %>%
      separate(bigram, c("word1", "word2"), sep = " ") 
wow_bigrams #separating them out for easier cleaning
```

```{r}
wow_bigrams <- wow_bigrams %>%
      filter(!word1 %in% stop_words$word) %>%
      filter(!word2 %in% stop_words$word)
wow_bigrams #filtering out unwanted words
```

```{r}
wow_bigrams <- wow_bigrams %>%
            filter(!is.na(word1)) %>% 
            filter(!is.na(word2))
wow_bigrams #removing null values
```

```{r}
wow_bigrams <- wow_bigrams %>%
            unite(bigram, word1, word2, sep=" ")
wow_bigrams #joining the words back together
```

```{r}
wow_bigram_counts <- wow_bigrams %>% count(bigram, sort=TRUE)
wow_bigram_counts #counting and sorting bigrams
```

```{r}
wow_bigram_counts %>% 
  filter(str_detect(wow_bigram_counts$bigram,"[0-9]", negate = TRUE)) -> wow_bigram_counts
wow_bigram_counts #removing any bigrams with numbers
```

```{r}
wow_filtered_bigrams <- wow_bigram_counts %>%
                  filter(n >= 4)
wow_filtered_bigrams #sorting remaining bigrams with a frequency of 4 or more
```

```{r}
wow_separated_bigrams <- wow_filtered_bigrams %>% 
  select("bigram") %>%
  separate(bigram, c("word1", "word2"), sep = " ") 
wow_separated_bigrams #separating bigrams again, preparing for graphical representation
```

```{r}
wow_bigram_graph <- wow_separated_bigrams %>%
                  graph_from_data_frame()
wow_bigram_graph #creating bigram graph
```

```{r}
ggraph(wow_bigram_graph, layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1)
```

## Step 5: Predictive modelling

### Initial setup
```{r}
train_balanced$is_bullying = as.factor(train_balanced$is_bullying)
test_balanced$is_bullying = as.factor(test_balanced$is_bullying)
train_control = trainControl(method = "cv", number = 5)
```

### SVM
```{r}
set.seed(42)

tic()

svm_model = caret::train(is_bullying~., data=train_balanced , method =  "svmLinear" , trControl = train_control)

svm_toc <- toc(quiet=T)

svm_time_taken <- svm_toc$toc - svm_toc$tic
svm_pred_y = predict(svm_model, test_balanced)
```

### MLP
```{r}
set.seed(42)
tic()

mlp_model = caret::train(is_bullying~., data=train_balanced , method =  "mlp" , trControl = train_control)
mlp_toc <- toc(quiet=T)

mlp_time_taken <- mlp_toc$toc - mlp_toc$tic
mlp_pred_y = predict(mlp_model, test_balanced)
```

### Random Forest
```{r}
set.seed(42)

tic()

rf_model = caret::train(is_bullying~., data=train_balanced , method =  "rf" , trControl = train_control)

rf_toc <- toc(quiet=T)

rf_time_taken <- rf_toc$toc - rf_toc$tic
rf_pred_y = predict(rf_model, test_balanced)
```

## Step 7: Hyperparameter Tuning
```{r}
train_control_grid = caret::trainControl(method = "cv", number = 5, search = "grid")
```
### SVM Linear kernel
```{r}
set.seed(50)
# Customizing the tuning grid
svm_linear_grid <-  expand.grid(C = c(0.1, 1, 10, 50, 100, 500, 1000))

#Setting the df as a factor
train_df$is_bullying = as.factor(train_df$is_bullying)

# training a svmLinear classification model while tuning parameters
svm_linear_tuned = caret::train(is_bullying~ ., data = train_df, method = "svmLinear", trControl = train_control_grid, tuneGrid = svm_linear_grid)

# summarising the results
print(svm_linear_tuned)
```

### SVM Polynomial kernel
```{r}
set.seed(50)
# Customsing the tuning grid
svm_poly_grid <-  expand.grid(degree = 4,
                            scale = 1,
                            C = c(0.1, 1, 10, 50, 100, 500, 1000))

#Setting the df as a factor
train_df$is_bullying = as.factor(train_df$is_bullying)

# training a svmLinear classification model while tuning parameters
svm_model_poly = caret::train(is_bullying~ ., data = train_df, method = "svmPoly", trControl = train_control_grid, tuneGrid = svm_poly_grid)

# summarising the results
print(svm_model_poly)
```

### SVM RBF Kernel
```{r}
# SVM Hyperparameter Tuning (polynomial kernel)

#Setting control grid method
train_control = caret::trainControl(method = "cv", number = 5, search = "grid")

set.seed(50)
# Customizing the tuning grid
svm_rbf_grid <-  expand.grid(sigma = 1,
                            C = c(0.1, 1, 10, 50, 100, 500, 1000))

#Setting the df as a factor
train_df$is_bullying = as.factor(train_df$is_bullying)

# training a svmLinear classification model while tuning parameters
svm_model_rbf = caret::train(is_bullying~ ., data = train_df, method = "svmRadial", trControl = train_control_grid, tuneGrid = svm_rbf_grid)

# summarizing the results
print(svm_model_rbf)
```

### MLP Tuned
```{r}
# MLP Hyperparameter Tuning
set.seed(50)
# Customizing the tuning grid
mlpGrid <-  expand.grid(size = 1)

#Setting the df as a factor
train_df$is_bullying = as.factor(train_df$is_bullying)

# training a svmLinear classification model while tuning parameters
mlp_model_tuned = caret::train(is_bullying~ ., data = train_df, method = "mlp", trControl = train_control, tuneGrid = mlpGrid)

# summarising the results
print(mlp_model_tuned)
```

## Step 7: Evaluation
### Confusion matrices
```{r}
svm_confusion_matrix <- caret::confusionMatrix(data=svm_pred_y, reference=test_balanced$is_bullying, mode="everything")
mlp_confusion_matrix <- caret::confusionMatrix(data=mlp_pred_y, reference=test_balanced$is_bullying, mode="everything")
rf_confusion_matrix <- caret::confusionMatrix(data=rf_pred_y, reference=test_balanced$is_bullying, mode="everything")
```
Here we create confusion matrices to get values such as F1 score, precision and recalls. When we use "mode="everything" ", we get additional values such as F1 Score for a better overview of the model performance.

```{r}
svm_confusion_matrix
```

```{r}
mlp_confusion_matrix
```

```{r}
rf_confusion_matrix
```

### ROC Curves
```{r}
converted_pred_y_svm <- as.numeric(levels(svm_pred_y))[svm_pred_y]
converted_pred_y_mlp <- as.numeric(levels(mlp_pred_y))[mlp_pred_y]
converted_pred_y_rf <- as.numeric(levels(rf_pred_y))[rf_pred_y]

par(pty="s")
svm_roc <- roc(test_balanced$is_bullying~converted_pred_y_svm, plot=TRUE, print.auc=TRUE, col="red", lwd=4, legacy.axes=TRUE, main="ROC Curves")
mlp_roc <- roc(test_balanced$is_bullying~converted_pred_y_mlp, plot=TRUE, print.auc=TRUE, print.auc.y=0.4, col="blue", lwd=4, legacy.axes=TRUE, add=TRUE)
rf_roc <- roc(test_balanced$is_bullying, converted_pred_y_rf, plot=TRUE, print.auc=TRUE, print.auc.y=0.6, col="green", lwd=4, legacy.axes=TRUE, add=TRUE)

legend("bottomright", legend=c("SVM", "MLP", "RF"), col=c("red", "blue", "green"), lwd=4)
```
This code creates ROC curves for each model with different colours. We then print the AUC values for each curve.

### Table of metrics
#### Calculating metrics
For each model, we will be calculating the evaluation metrics we will be using (time taken, precision, sensitivity, f1 score, AUC) to create a table of metrics which can be used to evaluate each model. We are acquiring the time taken to train each model to see how practical the models would be in a real world project.

```{r}
svm_precision <- precision(svm_pred_y, test_balanced$is_bullying)
svm_sensitivity <- sensitivity(svm_pred_y, test_balanced$is_bullying)
svm_f1_score <- F_meas(svm_pred_y, test_balanced$is_bullying)
svm_auc <- auc(svm_roc)
```

```{r}
mlp_precision <- precision(mlp_pred_y, test_balanced$is_bullying)
mlp_sensitivity <- sensitivity(mlp_pred_y, test_balanced$is_bullying)
mlp_f1_score <- F_meas(mlp_pred_y, test_balanced$is_bullying)
mlp_auc <- auc(mlp_roc)
```

```{r}
rf_precision <- precision(rf_pred_y, test_balanced$is_bullying)
rf_sensitivity <- sensitivity(rf_pred_y, test_balanced$is_bullying)
rf_f1_score <- F_meas(rf_pred_y, test_balanced$is_bullying)
rf_auc <- auc(rf_roc)
```

#### Table
```{r}
Model_Name <- c("SVM", "MLP", "RF")
Precision <- c(svm_precision, mlp_precision, rf_precision)
Sensitivity <- c(svm_sensitivity, mlp_sensitivity, rf_sensitivity)
F1_Score <- c(svm_f1_score, mlp_f1_score, rf_f1_score)
AUC <- c(svm_auc, mlp_auc, rf_auc)
Time_to_Train_secs <- c(svm_time_taken, mlp_time_taken, rf_time_taken)

results <- data.frame(Model_Name, Precision, Sensitivity, F1_Score, AUC, Time_to_Train_secs)
results
```

### Output evaluation results
```{r}
write.csv(results, "Data/results.csv")
```
We output the table results to a csv file to make it easy to share results with other group members.

# Individual contributions
## Thomas Newton
INSERT HERE
## Leon Harper
INSERT HERE
## Michal Jedruszczak
In this project I was responsible for the preliminary EDA and the final EDA. I also created the contents section and made sure that all of the references were in MMU Harvard format.

# References
Bretschneider, U. and Peters, R. "Detecting Cyberbullying in Online Communities" (2016). *Research Papers*. Paper 61. <http://aisel.aisnet.org/ecis2016_rp/61>
Hate is No Game: Harassment and Positive Social Experiences in Online Games 2021 (2022) ADL. Anti-Defamation League. Available at: https://www.adl.org/resources/report/hate-no-game-harassment-and-positive-social-experiences-online-games-2021 (Accessed: November 19, 2022). 
